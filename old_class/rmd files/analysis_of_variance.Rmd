---
title: "PSY 3307"
subtitle: "Analysis of Variance (ANOVA)"
author: "Jonathan A. Pedroza PhD"
institute: "Cal Poly Pomona"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

options(scipen = 99999)

library(tidyverse)
library(ggmap)
library(maps)
library(RColorBrewer)
library(ggrepel)
library(gganimate)
library(agricolae)
```

# Agenda

- Review t-tests
- Overview of Analysis of Variance (ANOVA)
  - Quick Review of all the different types of ANOVA
    + One-way ANOVA
    + Two-way ANOVA
    + Factorial ANOVA
    + Repeated-measures ANOVA
    + Mixed-effect ANOVA
- Components of ANOVA
- Performing ANOVA
- Post-hoc Tests (Tukey's HSD Test)
- Effect Size and Eta^2
- JP is Including More

---

# Review t-tests

- Things we need to remember
  - How to read a table (z, t, and now F)
  - Differences between within- and between-designs
  - What is an IV, DV, and conditions

---

# Analysis of Variance (ANOVA)

- **Factor** is just another word for IV

- A **level** is the same thing as a condition (from t-tests) and similar to a t-test, ANOVA has to do with differences between or among sample means
  - however ANOVA does not have restrictions on the number of groups we can test

- `k` is the symbol for the number of levels in a factor
  - otherwise known as the number of conditions in an IV

$$k = number\;of\;levels\;in\;factor$$

---

# Example of the Absurdity 

```{r, echo = FALSE, eval = TRUE}

data <- read_csv("https://raw.githubusercontent.com/jpedroza1228/dissertation/master/final_data/county20_sub.csv") %>% 
  janitor::clean_names() %>% 
  mutate(state_abbreviation = factor(state_abbreviation),
         state_abbreviation = relevel(state_abbreviation, ref = "CA")) %>% 
  filter(state_abbreviation != "US")
```

---

```{r}
aov_find <- aov(adult_smoking ~ state_abbreviation, data = data)
summary(aov_find)
```

---

```{r}
tukey_find <- TukeyHSD(aov_find)
tukey_find$`state_abbreviation`[1:10, 1:4]
```

---

# Another Reason Why I Like Regression

- In a real-life scenario, you would already have a hypothesis where you would be interested in whether or not a state is different from the rest

- Therefore, you'd already have a reference group to compare everyone to and just need to run one test

- Below is the same test, just through a regression framework

---

```{r}
lm_find <- lm(adult_smoking ~ state_abbreviation, data = data)
summary(lm_find)

```

---

```{r}
broom::tidy(lm_find)
```


---

```{r, echo = FALSE, eval = TRUE}
data %>% 
  mutate(ca_only = if_else(state_abbreviation == "CA", 1, 0),
         ca_only = factor(ca_only)) %>% 
ggplot(aes(fct_reorder(state_abbreviation, adult_smoking), adult_smoking)) + 
  geom_boxplot(aes(fill = ca_only)) +
  theme_minimal() +
  coord_flip() +
  theme(legend.position = "none")
```

---

# Breaking Down A One-way ANOVA

- **One-way ANOVA** is when we have an IV that has multiple levels (3+)
  - **NOTE** if you were to go on to SPSS and run a one-way ANOVA with the `sex` variable you would get the same answer
  - Essentially the same test being run

- Similar to a t-test, this also has within- and between-subjects designs

- Now instead of a t-table, we will be using a F-table

---

# We Are Now Working With Modeling

- There's just one problem. We have to work with ANOVA modeling

- **ANOVA** is a parametric procedure for dterming whether significant differences occur in an experiment containing two or more sample means

$$X_{ij} = \mu + \gamma_{j} + \epsilon_{ij}$$

- mu is the grand mean
- gamma is the specific treatment effect for group j (which group you are interested in looking at)
- epsilon if the error/residual of a specific individual (how much an individual deviates from the group's mean)

---

# Assumptions

- **homogeneity of variance** is the assumption that each population has the same variance

$$\sigma^2_{1} = \sigma^2_{2} = \sigma^2_{3} = ...$$

- **error variance** variance unrelated to any treatment differences

- **heterogeneity of variance** is when populations have different variances

- **normality** DV values are normally distributed

- **independence** observations are independent of one another
  - it really is that the residual/error is independent but for now we'll keep it as observations are different from one another

- The `n` in each level doesn't have to be exactly the same but they should not be drastically different

---

# Controlling Experiment-Wise Error Rate

- I had mentioned this previously that multiple independent-samples t-tests could do the same thing as a one-way ANOVA

- **experiment-wise error rate** is the probability of making a Type I error when comparing all means in an experiment

- with an F-test we are less likely to commit a type I error because we are not running all the tests possible

---

# Example

```{r}
jp <- rio::import(here::here("jp_thesis_1.sav")) %>% 
  janitor::clean_names() %>% 
  rowid_to_column() %>% 
  rename(sex = ccc_gender)
```

---

```{r, echo = FALSE, eval = TRUE}
jp %>% 
  mutate(sex = recode(sex, "1" = "Male",
                      "2" = "Female")) %>% 
  ggplot(aes(factor(sex), ccc_bmi)) + 
  geom_boxplot(aes(fill = factor(sex))) +
  labs(x = "",
       y = "Body Mass Index") + 
  theme(legend.position = "none")
```

---

```{r}
bmi_aov <- aov(ccc_bmi ~ factor(sex), data = jp)
summary(bmi_aov)
TukeyHSD(bmi_aov)

```

---

```{r}
t.test(ccc_bmi ~ factor(sex), data = jp, var.equal = TRUE)
```

---

# Steps to Conduct ANOVA

- Hypotheses
  - Similar to the independent-samples t-test, a one-way ANOVA's null hypothesis states that there is no difference between the levels/conditions
  - We just have more levels in a one-way ANOVA. 
  
- our null hypothesis would be

$$H0: \mu_1 = \mu_2 = \mu_3$$

$$H1: not\;all\;\mu\;are\;equal$$

- our alternative/research hypothesis is not that all groups would be significantly different from one another

- **JP Note** another option is to state that one group will be significantly different from the other groups

---

# F statistic

- Steps to ANOVA

1. Compute a F-statistic

2. Conduct a post-hoc test

---

# F-statistic

- we compute a F-statistic to see if any means are different
  - Significant F-statistic then there are differences somewhere between the multiple levels
  - Non-significant F-statistic means there are no differences between any levels

- The F-statistic only tells us whether or not a significant difference is found between any of the levels

- F-obtained is compared to a F-critical value to find statistical significance

---

# Post-hoc Tests OR Planned Comparisons

- Post-hoc often refers to after the fact

- Post-hoc tests are often considered after finding a statistically significant F-statistic

- like t-tests when comparing all combinations of each levels to see if there differences between two specific levels

- Planned comparisons are when you are interested in having a specific levels being different from the other levels

---

# Post-hoc Tests

- only look at post-hoc findings if there is a significant F-statistic

- no post-hoc tests are run when you only have two levels

---

# Different Types of Post-hoc Tests

- **Tukey Test or Tukey's HSD (Honestly Significant Difference)**

- Fisher's Least Significant Difference (LSD) Procedure

- Newman-Keuls Test

- Scheffe Test

- Dunnett's Test

- Benjamini-Hochberg Test

- **Bonferroni Test/Correction**
  - whatever alpha you are using, you then divide by the number of tests you conduct
  - alpha = .05, and you run 20 tests
  
--

- We'll get to these in the upcoming weeks (if only shortly)

---

# Bonferroni Correction Examples

- Alpha = .05, number of tests = 20
  -new p-value should be .0025

```{r}
.05/20

```

- alpha = .05, number of tests = 10
  - new p-value = _____

---

# Contrasts

- **a priori comparison** or before data is collected
  - also called **contrasts**

- **post hoc comparison** after data has been collected, looked at descriptive statistics, and somtimes...
  - they've looked at their test results
  
- gets complicated with the type of contrast chosen
  - linear contrasts
  - orthogonal contrasts

---

# Unequal Sample Sizes

- ANOVA is robust to slightly unequal sample sizes
  - like the example we did in SPSS
  
- **balanced designs** are when each level has the same amount of participants

- formula corrections are needed for unequal samples (see below)

- another option to make sure your levels are all equal in number of observations (n) is to fill in potential missing data
  - old-school methods include changing missing values to the mean or median
  - new-school methods include things like full-information maximum likelihood or multiple imputation (real fancy stuff)

---

# Componenets of ANOVA

$$S^2_{x} = \frac{\Sigma(X - \overline{X})^2}{N - 1}$$

$$S^2_{X} = \frac{\Sigma X^2 - \frac{(\Sigma X)^2}{N}}{N - 1}$$

$$S^2_{x} = \frac{Sum\;of\;Squares\;(SS)}{Degrees\;of\;Freedom\;(df)}$$

---

# Terminology

- Sum of Squares is broken down into three different categories
  - Sum of Squares Total
  - Sum of Squares Between/Treatment
  - Sum of Squares Within/Error
  
$$SS_{total}$$
$$SS_{bn} = SS_{treat}$$

$$SS_{wn} = SS_{error}$$
  
---

- `Sum of squares (SS)`/`degrees of freedom (df)` is equal to `mean square`

- `mean square` is often seen as `MS`

- when referring to ANOVA, `sum of the squared deviations` is called `sum of squares`

- this calculation of sum of squares divided by the degrees of freedom is called `mean square` or `MS`

- so our variance calculation is really our mean square in ANOVA
  - this is because we are calculating the `mean square within groups` and `mean square between groups`
  
---

# Formulas for Sum of Squares

$$SS_{total} = \Sigma(X_{ij} - \overline{X})^2$$

- This means that every value (i) within each level/group/condition (j) is put into this equation and then subtracting the `grand mean`

- **the grand mean** is the mean of all the observations of each level or the mean of each level's mean

$$SS_{treat} = n \Sigma(\overline{X_{j}} - \overline{X})^2$$

$$SS_{error} = SS_{total} - SS_{treat}$$

- Sum of squares error is actually the sum of each score within each level minus the mean for that level and then added all together to get the final value

---

$$SS_{error} = \Sigma(X_{ij} - \overline{X_{j}})^2$$

$$SS_{wn\;level1} = \Sigma((first\;observation - level\;1\;mean)^2 + (second\;observation - level\;1\;mean)^2 + ...$$

---

# Unequal Sample Size Correction

$$SS_{treat} = n \Sigma(\overline{X_{j}} - \overline{X})^2$$

will become 

$$SS_{treat} = \Sigma[n_{j}(\overline{X_{j}} - \overline{X})^2]$$

so this would then include the n for each group/level/condition

---

# Mean Square Within Groups

- **MS within groups** describes the variability in scores within the conditions/levels of an experiment

$$MS_{wn}$$

- we find differences among values in each level/condition and "pool" them together

- **MS within groups** is the "average" variability of scores within each level

- it is essentially a measure of variability of individual scores

---

# Mean Square Between Groups

- **MS between groups** is the differences between the means of each condition/level in a factor/IV

$$MS_{bn}$$

- measure difference between the means by treating them as scores, with an "average" amount they deviate from their mean, which in this case is the overall mean of the experiment

- similar to how scores deviate from a mean, this is a measure of the deviations of sample means from the overall mean

---

# The F-ratio

- MS between groups tells us whether or not the levels differ from one another and support our null/alternative hypotheses

- MS within groups estimates variability of individual scores

- if working with one population, our MS between should equal our MS within

- so if our null is true the MS between should be the same answer as the MS within

- **F-ratio** is a fraction consisting of MS between divided by the MS within or

$$F_{obt} = \frac{MS_{bn}}{MS_{wn}}$$

---

# F-ratio

- while it is unlikely that an exact value of 1 would be a F-obtained value, a value around 1 should be supportive of the null hypothesis

- the larger the F-ratio the more likely that the result is from sampling error or from the IV

- if our F-obtained value is larger than the F-critical value then we reject the null and accept the alternative/research hypothesis

---

# Performing an ANOVA

- before beginning, its important to note that for many of the calculations symbols will be similar like `MS` but it is important to note the subscripts

$$MS_{bn} \neq MS_{wn}$$

$$\left[\begin{array}{cc} 
Source\\
Between\\
Within\\
Total
\end{array}\right] 
\left[\begin{array}{cc} 
Sum\;of\;Squares\\
SS_{bn}\\
SS_{wn}\\
SS_{total}\\
\end{array}\right] 
\left[\begin{array}{cc} 
df\\
df_{bn}\\
df_{wn}\\
df_{total}\\
\end{array}\right] 
\left[\begin{array}{cc} 
Mean Square\\
MS_{bn}\\
MS_{wn}\\
\\
\end{array}\right] 
\left[\begin{array}{cc} 
F-ratio\\
F_{obt}\\
\\
\\
\end{array}\right]$$

---

# Computing for a F-obtained value

Steps for getting the F-obtained value

1. Calculate the sum of squares

2. Calculate the degrees of freedom

3. Calculate the mean squares

4. Calculate the F-obtained value

---

```{r}
difficulty <- data.frame(easy = c(9, 12, 4, 8, 7),
                         medium = c(4, 6, 8, 2, 10),
                         hard = c(1, 3, 4, 5, 2))
```

---

```{r}
easy_sum = 9+12+4+8+7
med_sum = 4+6+8+2+10
hard_sum = 1+3+4+5+2

easy_sum
med_sum
hard_sum
```

---

```{r}
easy_sum2 = 9^2+12^2+4^2+8^2+7^2
med_sum2 = 4^2+6^2+8^2+2^2+10^2
hard_sum2 = 1^2+3^2+4^2+5^2+2^2

easy_sum2
med_sum2
hard_sum2
```

---

```{r}
easy_n = 5
med_n = 5
hard_n = 5

easy_mean = easy_sum/easy_n
med_mean = med_sum/med_n
hard_mean = hard_sum/hard_n

easy_mean
med_mean
hard_mean

```

---

# Total Sum of all the Values

```{r}
total_sum = easy_sum + med_sum + hard_sum
total_sum
```

---

# Total Sum of all the Squared Values

```{r}
total_sum2 = easy_sum2 + med_sum2 + hard_sum2
total_sum2
```

---

# Total N

```{r}
total_n = easy_n + med_n + hard_n
total_n
```

---

# k or the number of levels/conditions

```{r}
k = 3
k
```

---

# Computing the Sums of Squares Total

$$SS_{total} = \Sigma X^2_{total} - \frac{(\Sigma X_{total})^2}{N}$$

$$SS_{total} = 629 - \frac{(85)^2}{15}$$

---

```{r}
85^2
```

$$SS_{total} = 629 - \frac{7225}{15}$$

---

```{r}
7225/15
```


$$SS_{total} = 629 - 481.67$$

---

```{r}
629 - 481.67
```

$$SS_{total} = 147.33$$

---

# Filling in the Table

$$\left[\begin{array}{cc} 
Source\\
Between\\
Within\\
Total
\end{array}\right] 
\left[\begin{array}{cc} 
Sum\;of\;Squares\\
SS_{bn}\\
SS_{wn}\\
147.33\\
\end{array}\right] 
\left[\begin{array}{cc} 
df\\
df_{bn}\\
df_{wn}\\
df_{total}\\
\end{array}\right] 
\left[\begin{array}{cc} 
Mean Square\\
MS_{bn}\\
MS_{wn}\\
\\
\end{array}\right] 
\left[\begin{array}{cc} 
F-ratio\\
F_{obt}\\
\\
\\
\end{array}\right]$$

---

# Sums of Squares Between

$$SS_{bn} = \Sigma(\frac{(\Sigma X\;in\;each\;column)^2}{n\;in\;each\;column}) - \frac{(\Sigma X_{total})^2}{N}$$

$$SS_{bn} = \Sigma(\frac{(40)^2}{5} + \frac{(30)^2}{5} + \frac{(15)^2}{5}) - \frac{(85)^2}{15}$$

---

```{r}
40^2

30^2

15^2

85^2
```

$$SS_{bn} = \Sigma(\frac{1600}{5} + \frac{900}{5} + \frac{225}{5}) - \frac{7225}{15}$$

---

```{r}
1600/5

900/5

225/5

7225/15
```

$$SS_{bn} = (320 + 180 + 45) - 481.67$$

---

```{r}
(320 + 180 + 45) - 481.67
```


$$SS_{bn} = 63.33$$

---

# Filling in the Table

$$\left[\begin{array}{cc} 
Source\\
Between\\
Within\\
Total
\end{array}\right] 
\left[\begin{array}{cc} 
Sum\;of\;Squares\\
63.33\\
SS_{wn}\\
147.33\\
\end{array}\right] 
\left[\begin{array}{cc} 
df\\
df_{bn}\\
df_{wn}\\
df_{total}\\
\end{array}\right] 
\left[\begin{array}{cc} 
Mean Square\\
MS_{bn}\\
MS_{wn}\\
\\
\end{array}\right] 
\left[\begin{array}{cc} 
F-ratio\\
F_{obt}\\
\\
\\
\end{array}\right]$$

---

# Sum of Squares Within Groups

$$SS_{wn} = SS_{total} - SS_{bn}$$

```{r}
147.33 - 63.33
```

---

# Using the other way

$$SS_{total} = \Sigma(X_{ij} - \overline{X})^2$$

$$SS_{treat} = n \Sigma(\overline{X_{j}} - \overline{X})^2$$

$$SS_{error} = SS_{total} - SS_{treat}$$

---

```{r}
difficulty

grand_mean = (9 + 12 + 4 + 8 + 7 + 4 + 6 + 8 + 2 + 10 + 1 + 3 + 4 + 5 + 2)/total_n
grand_mean

(easy_mean + med_mean + hard_mean)/3
```

---

```{r}
ss_total = (9 - grand_mean)^2 + (12 - grand_mean)^2 + (4 - grand_mean)^2 + (8 - grand_mean)^2 + (7 - grand_mean)^2 +
  (4 - grand_mean)^2 + (6 - grand_mean)^2 + (8 - grand_mean)^2 + (2 - grand_mean)^2 + (10 - grand_mean)^2 +
  (1 - grand_mean)^2 + (3 - grand_mean)^2 + (4 - grand_mean)^2 + (5 - grand_mean)^2 + (2 - grand_mean)^2

ss_total
```

---

```{r}
ss_treat = 5*((easy_mean - grand_mean)^2 + (med_mean - grand_mean)^2 + (hard_mean - grand_mean)^2)

ss_treat
```

---

```{r}
ss_error = ss_total - ss_treat

ss_error
```


---

# Filling in the Table

$$\left[\begin{array}{cc} 
Source\\
Between\\
Within\\
Total
\end{array}\right] 
\left[\begin{array}{cc} 
Sum\;of\;Squares\\
63.33\\
84\\
147.33\\
\end{array}\right] 
\left[\begin{array}{cc} 
df\\
df_{bn}\\
df_{wn}\\
df_{total}\\
\end{array}\right] 
\left[\begin{array}{cc} 
Mean Square\\
MS_{bn}\\
MS_{wn}\\
\\
\end{array}\right] 
\left[\begin{array}{cc} 
F-ratio\\
F_{obt}\\
\\
\\
\end{array}\right]$$

---

# Calculating degrees of freedom

- df between groups is simply the number of `groups/levels/conditions - 1`

$$df_{bn} = k - 1$$

```{r}
3 - 1
```

- df within groups is `N - k`

```{r}
15 - 3
```

- df total is still `N - 1`

```{r}
15 - 1
```



---

# Filling in the Table

$$\left[\begin{array}{cc} 
Source\\
Between\\
Within\\
Total
\end{array}\right] 
\left[\begin{array}{cc} 
Sum\;of\;Squares\\
63.33\\
84\\
147.33\\
\end{array}\right] 
\left[\begin{array}{cc} 
df\\
2\\
12\\
14\\
\end{array}\right] 
\left[\begin{array}{cc} 
Mean Square\\
MS_{bn}\\
MS_{wn}\\
\\
\end{array}\right] 
\left[\begin{array}{cc} 
F-ratio\\
F_{obt}\\
\\
\\
\end{array}\right]$$

---

# Computing the Mean Squares

$$MS_{bn} = \frac{SS_{bn}}{df_{bn}}$$

$$MS_{bn} = \frac{63.33}{2}$$

---

```{r}
63.33/2
```


$$MS_{bn} = 31.67$$

---

# Filling in the Table

$$\left[\begin{array}{cc} 
Source\\
Between\\
Within\\
Total
\end{array}\right] 
\left[\begin{array}{cc} 
Sum\;of\;Squares\\
63.33\\
84\\
147.33\\
\end{array}\right] 
\left[\begin{array}{cc} 
df\\
2\\
12\\
14\\
\end{array}\right] 
\left[\begin{array}{cc} 
Mean Square\\
31.67\\
MS_{wn}\\
\\
\end{array}\right] 
\left[\begin{array}{cc} 
F-ratio\\
F_{obt}\\
\\
\\
\end{array}\right]$$

---

# Computing the Mean Square within Groups

$$MS_{wn} = \frac{SS_{wn}}{df_{wn}}$$

$$MS_{wn} = \frac{84}{12}$$

---

```{r}
84/12
```

$$MS_{wn} = 7$$

---

# Filling in the Table

$$\left[\begin{array}{cc} 
Source\\
Between\\
Within\\
Total
\end{array}\right] 
\left[\begin{array}{cc} 
Sum\;of\;Squares\\
63.33\\
84\\
147.33\\
\end{array}\right] 
\left[\begin{array}{cc} 
df\\
2\\
12\\
14\\
\end{array}\right] 
\left[\begin{array}{cc} 
Mean Square\\
31.67\\
7\\
\\
\end{array}\right] 
\left[\begin{array}{cc} 
F-ratio\\
F_{obt}\\
\\
\\
\end{array}\right]$$

---

# Calculating the F-statistic

$$F_{obt} = \frac{MS_{bn}}{MS_{wn}}$$

$$F_{obt} = \frac{31.67}{7}$$

---

```{r}
31.67/7
```

$$F_{obt} =4.52$$

---

# Filling in the Table

$$\left[\begin{array}{cc} 
Source\\
Between\\
Within\\
Total
\end{array}\right] 
\left[\begin{array}{cc} 
Sum\;of\;Squares\\
63.33\\
84\\
147.33\\
\end{array}\right] 
\left[\begin{array}{cc} 
df\\
2\\
12\\
14\\
\end{array}\right] 
\left[\begin{array}{cc} 
Mean Square\\
31.67\\
7\\
\\
\end{array}\right] 
\left[\begin{array}{cc} 
F-ratio\\
4.52\\
\\
\\
\end{array}\right]$$

---

# Interpreting the F-obtained value

- **F-distribution** is the sampling distribution with values of F to represent when H0 is true and all conditions represent one population (no differences between groups/levels)

- F cannot be less than zero

- There is no limit to how large an F-obtained value can be

- the mean of the F-distribution is 1

- F-distribution shape also depends on the df

---

# F-table

- uses alpha, df within, df between

- line up the df within with the df between and choose the value based on the alpha decided on

- the F-table only tells us one thing, `is there a statistically significant difference between the means of the three groups/conditions/levels`

- in order to see which specific group comparisons are significantly different from one another, we will rely on post-hoc tests or examination of the contrasts

---

# Writing up Findings

- when reporting, you would first include the test that you ran and the context
  - how many groups, your IV, your DV

- you would then report the F statistic
  - F(df between, df within) = F obtained value, p value

- then you would report the post-hoc tests, which would include the group/level means that were statistically significant from one another

- lastly you would report the effect size/eta squared (we'll get to this next week)

---

# Next Class

- We'll talk about:
  - effect sizes
  - calculations for post-hoc tests
  - proportions of variance accounted for
  - slight introduction to what an ANCOVA is
  
---

Practice Problems

You are interested in ways your class (N = 30) can increase their happiness. You are testing two methods to improve happiness and one control group. Your groups are: 1 = having plants to take care of (n = 10), 2 = support animal (n = 10), 3 = control (n = 10). You are interested in there is a significant difference between these three groups on your students happiness. 

---

```{r}
set.seed(101421)

plants <- rnorm(10, mean = 80, sd = 11)
animal <- rnorm(10, mean = 40, sd = 8.4)
control <- rnorm(10, mean = 50, sd = 2.1)

happy <- data.frame(plants,
                   animal,
                   control)

happy

```

---

You are having a movie marathon of all your favorite spooky movies to watch. You decide to make a competition between you and your friends by seeing whose method of staying away longer will allow you to watch more movies in a weekend. You decide that you and some others (n = 5) are going to drink two pots of coffee each day to stay away all weekend. Another group (n = 5) decide that they are going to drink energy drinks to stay up. Your last group of friends (n = 5) are going to rely on each other to stay up by shaking each other awaken. You are interested in what group will stay up the latest. 

---

```{r}
set.seed(101421)

coffee <- rnorm(5, mean = 20, sd = 4.5)
energy <- rnorm(5, mean = 43, sd = 1.2)
support <- rnorm(5, mean = 10, sd = .5)

spooky <- data.frame(coffee,
                     energy,
                     support)

spooky

```

